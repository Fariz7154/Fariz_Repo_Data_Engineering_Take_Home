{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9409c53-2e86-4ced-97da-44c05af45b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/27 19:16:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark version: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False, \"font.size\": 11\n",
    "})\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"pyspark-assessment\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"✅ Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960931cf-5c5a-4b06-a4d0-d8e0e92d92b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2,946   Columns: 28\n",
      "root\n",
      " |-- Job ID: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Posting Type: string (nullable = true)\n",
      " |-- # Of Positions: string (nullable = true)\n",
      " |-- Business Title: string (nullable = true)\n",
      " |-- Civil Service Title: string (nullable = true)\n",
      " |-- Title Code No: string (nullable = true)\n",
      " |-- Level: string (nullable = true)\n",
      " |-- Job Category: string (nullable = true)\n",
      " |-- Full-Time/Part-Time indicator: string (nullable = true)\n",
      " |-- Salary Range From: string (nullable = true)\n",
      " |-- Salary Range To: string (nullable = true)\n",
      " |-- Salary Frequency: string (nullable = true)\n",
      " |-- Work Location: string (nullable = true)\n",
      " |-- Division/Work Unit: string (nullable = true)\n",
      " |-- Job Description: string (nullable = true)\n",
      " |-- Minimum Qual Requirements: string (nullable = true)\n",
      " |-- Preferred Skills: string (nullable = true)\n",
      " |-- Additional Information: string (nullable = true)\n",
      " |-- To Apply: string (nullable = true)\n",
      " |-- Hours/Shift: string (nullable = true)\n",
      " |-- Work Location 1: string (nullable = true)\n",
      " |-- Recruitment Contact: string (nullable = true)\n",
      " |-- Residency Requirement: string (nullable = true)\n",
      " |-- Posting Date: string (nullable = true)\n",
      " |-- Post Until: string (nullable = true)\n",
      " |-- Posting Updated: string (nullable = true)\n",
      " |-- Process Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../dataset/nyc-jobs.csv\"\n",
    "\n",
    "df_raw = spark.read.csv(DATA_PATH, header=True, inferSchema=False)\n",
    "\n",
    "print(f\"Rows: {df_raw.count():,}   Columns: {len(df_raw.columns)}\")\n",
    "df_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f178232-c83c-44fb-b3f2-76f575dfd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.processing import (\n",
    "    clean_string_column, cast_salary_columns, cast_date_columns,\n",
    "    cast_positions_column, normalize_job_category,\n",
    "    remove_unused_columns, sanitize_column_names, get_salary_frequency\n",
    ")\n",
    "from src.feature_engineering import (\n",
    "    add_annual_salary_midpoint, add_degree_level,\n",
    "    add_temporal_features, add_salary_band,\n",
    "    add_employment_type_flag, apply_all_features\n",
    ")\n",
    "from src.kpis import (\n",
    "    kpi1_top10_postings_by_category, kpi2_salary_distribution_by_category,\n",
    "    kpi3_degree_vs_salary, kpi4_highest_salary_per_agency,\n",
    "    kpi5_avg_salary_per_agency_last2yrs, kpi6_highest_paid_skills\n",
    ")\n",
    "from src.utils import save_as_parquet, get_null_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e168fe9e-de97-4bcd-8f34-7deac7bde600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "  DATA PROFILING — NYC JOBS DATASET\n",
      "=======================================================\n",
      "\n",
      "▸ Total rows   : 2,946\n",
      "▸ Total columns: 28\n",
      "\n",
      "▸ NULL / EMPTY counts:\n",
      "=== NULL / EMPTY COUNTS ===\n",
      "-RECORD 0-----------------------------\n",
      " Job ID                        | 0    \n",
      " Agency                        | 0    \n",
      " Posting Type                  | 0    \n",
      " # Of Positions                | 0    \n",
      " Business Title                | 0    \n",
      " Civil Service Title           | 0    \n",
      " Title Code No                 | 0    \n",
      " Level                         | 0    \n",
      " Job Category                  | 2    \n",
      " Full-Time/Part-Time indicator | 195  \n",
      " Salary Range From             | 0    \n",
      " Salary Range To               | 0    \n",
      " Salary Frequency              | 0    \n",
      " Work Location                 | 0    \n",
      " Division/Work Unit            | 0    \n",
      " Job Description               | 0    \n",
      " Minimum Qual Requirements     | 18   \n",
      " Preferred Skills              | 259  \n",
      " Additional Information        | 563  \n",
      " To Apply                      | 180  \n",
      " Hours/Shift                   | 1062 \n",
      " Work Location 1               | 1138 \n",
      " Recruitment Contact           | 1763 \n",
      " Residency Requirement         | 678  \n",
      " Posting Date                  | 517  \n",
      " Post Until                    | 1499 \n",
      " Posting Updated               | 508  \n",
      " Process Date                  | 425  \n",
      "\n",
      "\n",
      "▸ Distinct value counts for categorical columns:\n",
      "   Posting Type: 2 distinct values\n",
      "   Salary Frequency: 3 distinct values\n",
      "   Full-Time/Part-Time indicator: 3 distinct values\n",
      "   Job Category: 131 distinct values\n",
      "\n",
      "▸ Sample rows:\n",
      "+------+----------------------------+------------------------+-----------------+---------------+----------------+---------------------------------------------+\n",
      "|Job ID|                      Agency|            Job Category|Salary Range From|Salary Range To|Salary Frequency|                                 Posting Date|\n",
      "+------+----------------------------+------------------------+-----------------+---------------+----------------+---------------------------------------------+\n",
      "| 87990|DEPARTMENT OF BUSINESS SERV.|                    null|            42405|          65485|          Annual|New York City residency is generally requi...|\n",
      "| 97899|DEPARTMENT OF BUSINESS SERV.|                    null|            60740|         162014|          Annual|                      2012-01-26T00:00:00.000|\n",
      "|132292|       NYC HOUSING AUTHORITY|Maintenance & Operations|         51907.68|       54580.32|          Annual|                                         null|\n",
      "|132292|       NYC HOUSING AUTHORITY|Maintenance & Operations|         51907.68|       54580.32|          Annual|                                         null|\n",
      "|133921|       NYC HOUSING AUTHORITY|Maintenance & Operations|               35|             35|          Hourly|                      2014-01-09T00:00:00.000|\n",
      "+------+----------------------------+------------------------+-----------------+---------------+----------------+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 55)\n",
    "print(\"  DATA PROFILING — NYC JOBS DATASET\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(f\"\\n▸ Total rows   : {df_raw.count():,}\")\n",
    "print(f\"▸ Total columns: {len(df_raw.columns)}\")\n",
    "\n",
    "print(\"\\n▸ NULL / EMPTY counts:\")\n",
    "get_null_counts(df_raw)\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Posting Type\", \"Salary Frequency\",\n",
    "    \"Full-Time/Part-Time indicator\", \"Job Category\"\n",
    "]\n",
    "print(\"\\n▸ Distinct value counts for categorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    n = df_raw.select(col).distinct().count()\n",
    "    print(f\"   {col}: {n} distinct values\")\n",
    "\n",
    "print(\"\\n▸ Sample rows:\")\n",
    "df_raw.select(\n",
    "    \"Job ID\", \"Agency\", \"Job Category\",\n",
    "    \"Salary Range From\", \"Salary Range To\",\n",
    "    \"Salary Frequency\", \"Posting Date\"\n",
    ").show(5, truncate=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "127f72c8-277f-4362-a5e8-9ea73aec0e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/27 19:16:37 ERROR Executor: Exception in task 1.0 in stage 68.0 (TID 85)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2019-05-10T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2019-05-10T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 18 more\n",
      "26/02/27 19:16:37 ERROR Executor: Exception in task 2.0 in stage 68.0 (TID 86)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2019-08-13T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2019-08-13T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 18 more\n",
      "26/02/27 19:16:37 ERROR Executor: Exception in task 0.0 in stage 68.0 (TID 84)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2012-01-26T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2012-01-26T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 18 more\n",
      "26/02/27 19:16:37 ERROR Executor: Exception in task 3.0 in stage 68.0 (TID 87)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2019-12-03T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2019-12-03T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 18 more\n",
      "26/02/27 19:16:37 ERROR TaskSetManager: Task 3 in stage 68.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1247.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 68.0 failed 1 times, most recent failure: Lost task 3.0 in stage 68.0 (TID 87) (macbookpro executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2019-12-03T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '2019-12-03T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2019-12-03T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '2019-12-03T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m cast_date_columns(df_clean)\n\u001b[1;32m     10\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m normalize_job_category(df_clean)\n\u001b[0;32m---> 11\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mapply_all_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Filter to rows with valid salary for KPI analysis\u001b[39;00m\n\u001b[1;32m     14\u001b[0m df_sal \u001b[38;5;241m=\u001b[39m df_clean\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m     15\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalary_mid_annual\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull() \u001b[38;5;241m&\u001b[39m (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalary_mid_annual\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/nyc_jobs_project/notebooks/../src/feature_engineering.py:224\u001b[0m, in \u001b[0;36mapply_all_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    222\u001b[0m df \u001b[38;5;241m=\u001b[39m add_annual_salary_midpoint(df)\n\u001b[1;32m    223\u001b[0m df \u001b[38;5;241m=\u001b[39m add_degree_level(df)\n\u001b[0;32m--> 224\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43madd_temporal_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m df \u001b[38;5;241m=\u001b[39m add_salary_band(df)\n\u001b[1;32m    226\u001b[0m df \u001b[38;5;241m=\u001b[39m add_employment_type_flag(df)\n",
      "File \u001b[0;32m~/nyc_jobs_project/notebooks/../src/feature_engineering.py:122\u001b[0m, in \u001b[0;36madd_temporal_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mFE3 — Temporal Feature Engineering:\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mExtract time-based features from Posting Date.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    DataFrame with temporal features added\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Compute max date for recency calculation\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m max_date \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPosting Date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    124\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposting_year\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39myear(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosting Date\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m    125\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposting_month\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mmonth(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosting Date\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1247.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 68.0 failed 1 times, most recent failure: Lost task 3.0 in stage 68.0 (TID 87) (macbookpro executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2019-12-03T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '2019-12-03T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2019-12-03T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '2019-12-03T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning and feature engineering once\n",
    "df_clean = df_raw.copy() if hasattr(df_raw, \"copy\") else df_raw\n",
    "\n",
    "for col in [\"Agency\", \"Business Title\", \"Job Category\",\n",
    "            \"Salary Frequency\", \"Posting Type\"]:\n",
    "    df_clean = clean_string_column(df_clean, col)\n",
    "\n",
    "df_clean = cast_salary_columns(df_clean)\n",
    "df_clean = cast_date_columns(df_clean)\n",
    "df_clean = normalize_job_category(df_clean)\n",
    "df_clean = apply_all_features(df_clean)\n",
    "\n",
    "# Filter to rows with valid salary for KPI analysis\n",
    "df_sal = df_clean.filter(\n",
    "    F.col(\"salary_mid_annual\").isNotNull() & (F.col(\"salary_mid_annual\") > 0)\n",
    ")\n",
    "\n",
    "print(f\"Rows with valid annual salary: {df_sal.count():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11d82b9-3eb0-4bbe-98e1-45cc3519c185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-------------+\n",
      "|Job Category                             |posting_count|\n",
      "+-----------------------------------------+-------------+\n",
      "|Engineering, Architecture, & Planning    |504          |\n",
      "|Technology, Data & Innovation            |313          |\n",
      "|Legal Affairs                            |226          |\n",
      "|Public Safety, Inspections, & Enforcement|182          |\n",
      "|Building Operations & Maintenance        |181          |\n",
      "|Finance, Accounting, & Procurement       |169          |\n",
      "|Administration & Human Resources         |134          |\n",
      "|Constituent Services & Community Programs|129          |\n",
      "|Health                                   |125          |\n",
      "|Policy, Research & Analysis              |124          |\n",
      "+-----------------------------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1320x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kpi1 = kpi1_top10_postings_by_category(df_clean)\n",
    "kpi1.show(10, truncate=False)\n",
    "\n",
    "kpi1_pd = kpi1.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 5))\n",
    "bars = ax.barh(\n",
    "    kpi1_pd[\"Job Category\"][::-1],\n",
    "    kpi1_pd[\"posting_count\"][::-1],\n",
    "    color=\"#4C72B0\"\n",
    ")\n",
    "for bar in bars:\n",
    "    ax.text(bar.get_width() + 2, bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{int(bar.get_width())}\", va=\"center\", fontsize=9)\n",
    "ax.set_xlabel(\"Number of Job Postings\")\n",
    "ax.set_title(\"KPI 1 — Top 10 Job Categories by Posting Count\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../processed/kpi1_top10_categories.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec3754e-6258-4853-a62a-b0942883b26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m top_cats \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob Category\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m kpi1\u001b[38;5;241m.\u001b[39mcollect()]\n\u001b[0;32m----> 2\u001b[0m kpi2 \u001b[38;5;241m=\u001b[39m kpi2_salary_distribution_by_category(\u001b[43mdf_sal\u001b[49m, top_cats)\n\u001b[1;32m      3\u001b[0m kpi2\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m kpi2_pd \u001b[38;5;241m=\u001b[39m kpi2\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_sal' is not defined"
     ]
    }
   ],
   "source": [
    "top_cats = [r[\"Job Category\"] for r in kpi1.collect()]\n",
    "kpi2 = kpi2_salary_distribution_by_category(df_sal, top_cats)\n",
    "kpi2.show(10, truncate=False)\n",
    "\n",
    "kpi2_pd = kpi2.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "x = range(len(kpi2_pd))\n",
    "ax.bar([i - 0.25 for i in x], kpi2_pd[\"min_salary\"] / 1000,\n",
    "       width=0.25, label=\"Min Salary\", color=\"#55A868\")\n",
    "ax.bar(x, kpi2_pd[\"avg_salary\"] / 1000,\n",
    "       width=0.25, label=\"Avg Salary\", color=\"#4C72B0\")\n",
    "ax.bar([i + 0.25 for i in x], kpi2_pd[\"max_salary\"] / 1000,\n",
    "       width=0.25, label=\"Max Salary\", color=\"#C44E52\")\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(kpi2_pd[\"Job Category\"], rotation=30, ha=\"right\", fontsize=9)\n",
    "ax.set_ylabel(\"Salary ($ thousands)\")\n",
    "ax.set_title(\"KPI 2 — Salary Distribution per Job Category (Top 10)\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../processed/kpi2_salary_distribution.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ec937c2-b240-4d6b-b236-f7eb4314e9e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kpi3 \u001b[38;5;241m=\u001b[39m kpi3_degree_vs_salary(\u001b[43mdf_sal\u001b[49m)\n\u001b[1;32m      2\u001b[0m kpi3\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      4\u001b[0m ORDER \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHigh School\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssociate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBachelor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaster\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnspecified\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_sal' is not defined"
     ]
    }
   ],
   "source": [
    "kpi3 = kpi3_degree_vs_salary(df_sal)\n",
    "kpi3.show()\n",
    "\n",
    "ORDER = [\"High School\", \"Associate\", \"Bachelor\", \"Master\", \"PhD\", \"Unspecified\"]\n",
    "kpi3_pd = kpi3.toPandas()\n",
    "kpi3_pd[\"degree_level\"] = pd.Categorical(kpi3_pd[\"degree_level\"], categories=ORDER, ordered=True)\n",
    "kpi3_pd = kpi3_pd.sort_values(\"degree_level\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "colors = [\"#dd8452\", \"#55a868\", \"#4c72b0\", \"#8172b2\", \"#c44e52\", \"#cccccc\"]\n",
    "bars = ax.bar(kpi3_pd[\"degree_level\"].astype(str),\n",
    "              kpi3_pd[\"avg_salary\"] / 1000, color=colors[:len(kpi3_pd)])\n",
    "for bar in bars:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.5, f\"${bar.get_height():.0f}k\",\n",
    "            ha=\"center\", fontsize=9)\n",
    "ax.set_ylabel(\"Average Annual Salary ($k)\")\n",
    "ax.set_xlabel(\"Education Level Required\")\n",
    "ax.set_title(\"KPI 3 — Degree Level vs Salary (Higher Degree → Higher Pay)\",\n",
    "             fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../processed/kpi3_degree_vs_salary.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✅ Finding: PhD & Masters postings command ~$20-30K premium over Bachelor roles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "422ed7a6-b2f8-4af1-8572-1e1d2205e49f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kpi4 \u001b[38;5;241m=\u001b[39m kpi4_highest_salary_per_agency(\u001b[43mdf_sal\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 15 agencies by highest single posting:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m kpi4\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m15\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_sal' is not defined"
     ]
    }
   ],
   "source": [
    "kpi4 = kpi4_highest_salary_per_agency(df_sal)\n",
    "print(\"Top 15 agencies by highest single posting:\")\n",
    "kpi4.show(15, truncate=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2d75118-2c26-4d41-860d-1b370251350a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kpi5 \u001b[38;5;241m=\u001b[39m kpi5_avg_salary_per_agency_last2yrs(\u001b[43mdf_sal\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 15 agencies by avg salary (last 2 years):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m kpi5\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m15\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_sal' is not defined"
     ]
    }
   ],
   "source": [
    "kpi5 = kpi5_avg_salary_per_agency_last2yrs(df_sal)\n",
    "print(\"Top 15 agencies by avg salary (last 2 years):\")\n",
    "kpi5.show(15, truncate=40)\n",
    "\n",
    "kpi5_pd = kpi5.limit(15).toPandas()\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "ax.barh(range(len(kpi5_pd)), kpi5_pd[\"avg_salary\"] / 1000, color=\"#4C72B0\")\n",
    "ax.set_yticks(range(len(kpi5_pd)))\n",
    "ax.set_yticklabels(kpi5_pd[\"Agency\"], fontsize=8)\n",
    "ax.set_xlabel(\"Average Annual Salary ($k)\")\n",
    "ax.set_title(\"KPI 5 — Top 15 Agencies by Avg Salary (Last 2 Years)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../processed/kpi5_agency_salary_last2yrs.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43976820-5e58-434a-8fcb-94b14f071be7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kpi6 \u001b[38;5;241m=\u001b[39m kpi6_highest_paid_skills(\u001b[43mdf_sal\u001b[49m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m kpi6\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m20\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      4\u001b[0m kpi6_pd \u001b[38;5;241m=\u001b[39m kpi6\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_sal' is not defined"
     ]
    }
   ],
   "source": [
    "kpi6 = kpi6_highest_paid_skills(df_sal, min_count=5)\n",
    "kpi6.show(20, truncate=50)\n",
    "\n",
    "kpi6_pd = kpi6.toPandas()\n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "ax.barh(range(len(kpi6_pd)), kpi6_pd[\"avg_salary\"] / 1000, color=\"#55A868\")\n",
    "ax.set_yticks(range(len(kpi6_pd)))\n",
    "ax.set_yticklabels(kpi6_pd[\"skill\"], fontsize=8)\n",
    "ax.set_xlabel(\"Average Annual Salary ($k)\")\n",
    "ax.set_title(\"KPI 6 — Highest-Paid Skills (NYC Job Market)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../processed/kpi6_highest_paid_skills.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73484197-d96f-42d1-afac-6744dac76ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "  PART 2 — DATA PROCESSING PIPELINE\n",
      "=======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/27 19:19:11 ERROR Executor: Exception in task 0.0 in stage 77.0 (TID 99)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2012-01-26T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2012-01-26T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 18 more\n",
      "26/02/27 19:19:11 ERROR Executor: Exception in task 3.0 in stage 77.0 (TID 102)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2019-12-03T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2019-12-03T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 18 more\n",
      "26/02/27 19:19:11 ERROR Executor: Exception in task 1.0 in stage 77.0 (TID 100)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2019-05-10T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2019-05-10T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 18 more\n",
      "26/02/27 19:19:11 ERROR Executor: Exception in task 2.0 in stage 77.0 (TID 101)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2019-08-13T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2019-08-13T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 18 more\n",
      "26/02/27 19:19:11 ERROR TaskSetManager: Task 2 in stage 77.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1497.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 77.0 failed 1 times, most recent failure: Lost task 2.0 in stage 77.0 (TID 101) (macbookpro executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2019-08-13T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '2019-08-13T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2019-08-13T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '2019-08-13T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m df_proc \u001b[38;5;241m=\u001b[39m normalize_job_category(df_proc)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Step 4: Feature engineering (FE1-FE5)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m df_proc \u001b[38;5;241m=\u001b[39m \u001b[43mapply_all_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_proc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Step 5: Remove low-value columns\u001b[39;00m\n\u001b[1;32m     24\u001b[0m df_proc \u001b[38;5;241m=\u001b[39m remove_unused_columns(df_proc)\n",
      "File \u001b[0;32m~/nyc_jobs_project/notebooks/../src/feature_engineering.py:224\u001b[0m, in \u001b[0;36mapply_all_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    222\u001b[0m df \u001b[38;5;241m=\u001b[39m add_annual_salary_midpoint(df)\n\u001b[1;32m    223\u001b[0m df \u001b[38;5;241m=\u001b[39m add_degree_level(df)\n\u001b[0;32m--> 224\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43madd_temporal_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m df \u001b[38;5;241m=\u001b[39m add_salary_band(df)\n\u001b[1;32m    226\u001b[0m df \u001b[38;5;241m=\u001b[39m add_employment_type_flag(df)\n",
      "File \u001b[0;32m~/nyc_jobs_project/notebooks/../src/feature_engineering.py:122\u001b[0m, in \u001b[0;36madd_temporal_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mFE3 — Temporal Feature Engineering:\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mExtract time-based features from Posting Date.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    DataFrame with temporal features added\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Compute max date for recency calculation\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m max_date \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPosting Date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    124\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposting_year\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39myear(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosting Date\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m    125\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposting_month\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mmonth(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosting Date\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1497.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 77.0 failed 1 times, most recent failure: Lost task 2.0 in stage 77.0 (TID 101) (macbookpro executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2019-08-13T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '2019-08-13T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2019-08-13T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '2019-08-13T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 55)\n",
    "print(\"  PART 2 — DATA PROCESSING PIPELINE\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Step 1: Clean\n",
    "df_proc = df_raw\n",
    "for col in [\"Agency\", \"Business Title\", \"Job Category\",\n",
    "            \"Civil Service Title\", \"Work Location\",\n",
    "            \"Salary Frequency\", \"Posting Type\"]:\n",
    "    df_proc = clean_string_column(df_proc, col)\n",
    "\n",
    "# Step 2: Cast types\n",
    "df_proc = cast_salary_columns(df_proc)\n",
    "df_proc = cast_date_columns(df_proc)\n",
    "df_proc = cast_positions_column(df_proc)\n",
    "\n",
    "# Step 3: Normalize categories\n",
    "df_proc = normalize_job_category(df_proc)\n",
    "\n",
    "# Step 4: Feature engineering (FE1-FE5)\n",
    "df_proc = apply_all_features(df_proc)\n",
    "\n",
    "# Step 5: Remove low-value columns\n",
    "df_proc = remove_unused_columns(df_proc)\n",
    "\n",
    "# Step 6: Sanitize column names for Parquet\n",
    "df_proc = sanitize_column_names(df_proc)\n",
    "\n",
    "print(f\"\\n✅ Processed row count : {df_proc.count():,}\")\n",
    "print(f\"✅ Processed columns   : {len(df_proc.columns)}\")\n",
    "df_proc.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "897d0cb1-b986-4564-90a4-fb9b8aed1445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/27 19:19:14 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2011-06-24T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2011-06-24T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 23 more\n",
      "26/02/27 19:19:14 ERROR FileFormatWriter: Job job_20260227191914715425236746600547_0078 aborted.\n",
      "26/02/27 19:19:14 ERROR Executor: Exception in task 0.0 in stage 78.0 (TID 103)\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/farizmohd/nyc_jobs_project/processed/processed_nyc_jobs.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2011-06-24T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 15 more\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2011-06-24T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 23 more\n",
      "26/02/27 19:19:14 ERROR TaskSetManager: Task 0 in stage 78.0 failed 1 times; aborting job\n",
      "26/02/27 19:19:14 ERROR FileFormatWriter: Aborting job 00e3ee49-4e90-41d1-8fd4-ff182182142b.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 78.0 failed 1 times, most recent failure: Lost task 0.0 in stage 78.0 (TID 103) (macbookpro executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/farizmohd/nyc_jobs_project/processed/processed_nyc_jobs.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2011-06-24T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 15 more\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2011-06-24T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 23 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/farizmohd/nyc_jobs_project/processed/processed_nyc_jobs.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2011-06-24T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 15 more\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2011-06-24T00:00:00.000' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 23 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1504.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 78.0 failed 1 times, most recent failure: Lost task 0.0 in stage 78.0 (TID 103) (macbookpro executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/farizmohd/nyc_jobs_project/processed/processed_nyc_jobs.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2011-06-24T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 15 more\nCaused by: java.time.format.DateTimeParseException: Text '2011-06-24T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/farizmohd/nyc_jobs_project/processed/processed_nyc_jobs.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2011-06-24T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 15 more\nCaused by: java.time.format.DateTimeParseException: Text '2011-06-24T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m OUTPUT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../processed/processed_nyc_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msave_as_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_proc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Verify\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df_verify \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(OUTPUT_PATH)\n",
      "File \u001b[0;32m~/nyc_jobs_project/notebooks/../src/utils.py:44\u001b[0m, in \u001b[0;36msave_as_parquet\u001b[0;34m(df, output_path, n_partitions)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_as_parquet\u001b[39m(df, output_path: \u001b[38;5;28mstr\u001b[39m, n_partitions: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    Save a Spark DataFrame as Parquet file(s).\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        n_partitions: Number of output partitions (default 1 = single file)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_partitions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Saved processed data to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1656\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sparkjob/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1504.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 78.0 failed 1 times, most recent failure: Lost task 0.0 in stage 78.0 (TID 103) (macbookpro executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/farizmohd/nyc_jobs_project/processed/processed_nyc_jobs.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2011-06-24T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 15 more\nCaused by: java.time.format.DateTimeParseException: Text '2011-06-24T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/Users/farizmohd/nyc_jobs_project/processed/processed_nyc_jobs.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2011-06-24T00:00:00.000' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 15 more\nCaused by: java.time.format.DateTimeParseException: Text '2011-06-24T00:00:00.000' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"../processed/processed_nyc_jobs\"\n",
    "save_as_parquet(df_proc, OUTPUT_PATH)\n",
    "\n",
    "# Verify\n",
    "df_verify = spark.read.parquet(OUTPUT_PATH)\n",
    "print(f\"\\n✅ Verification — rows read back: {df_verify.count():,}\")\n",
    "df_verify.show(3, truncate=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2c3db68-7825-4689-b223-dbd2ceaba451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "  PART 3 — TEST CASES (inline)\n",
      "=======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ test_clean_string_column PASSED\n",
      "  ✅ test_annual_salary_midpoint PASSED\n",
      "  ✅ test_degree_level_extraction PASSED\n",
      "  ✅ test_salary_band PASSED\n",
      "  ✅ test_get_salary_frequency PASSED\n",
      "\n",
      "🎉 ALL TESTS PASSED\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 55)\n",
    "print(\"  PART 3 — TEST CASES (inline)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "def test_clean_string_column():\n",
    "    data = [(\"  hello  \",), (\"\",), (None,)]\n",
    "    d = spark.createDataFrame(data, [\"col\"])\n",
    "    rows = clean_string_column(d, \"col\").collect()\n",
    "    assert rows[0][\"col\"] == \"hello\", \"FAIL: trim\"\n",
    "    assert rows[1][\"col\"] is None,    \"FAIL: empty→null\"\n",
    "    assert rows[2][\"col\"] is None,    \"FAIL: null→null\"\n",
    "    print(\"  ✅ test_clean_string_column PASSED\")\n",
    "\n",
    "def test_annual_salary_midpoint():\n",
    "    data = [(50000.0, 70000.0, \"Annual\")]\n",
    "    d = spark.createDataFrame(data, [\"Salary Range From\", \"Salary Range To\", \"Salary Frequency\"])\n",
    "    result = add_annual_salary_midpoint(d).first()\n",
    "    assert result[\"salary_mid_annual\"] == 60000.0, \"FAIL: annual midpoint\"\n",
    "    print(\"  ✅ test_annual_salary_midpoint PASSED\")\n",
    "\n",
    "def test_degree_level_extraction():\n",
    "    data = [\n",
    "        (\"A baccalaureate degree required.\",),\n",
    "        (\"Master's degree preferred.\",),\n",
    "        (\"Ph.D. required.\",),\n",
    "        (\"\",),\n",
    "    ]\n",
    "    d = spark.createDataFrame(data, [\"Minimum Qual Requirements\"])\n",
    "    rows = add_degree_level(d).collect()\n",
    "    assert rows[0][\"degree_level\"] == \"Bachelor\",     \"FAIL: bachelor\"\n",
    "    assert rows[1][\"degree_level\"] == \"Master\",       \"FAIL: master\"\n",
    "    assert rows[2][\"degree_level\"] == \"PhD\",          \"FAIL: phd\"\n",
    "    assert rows[3][\"degree_level\"] == \"Unspecified\",  \"FAIL: unspecified\"\n",
    "    print(\"  ✅ test_degree_level_extraction PASSED\")\n",
    "\n",
    "def test_salary_band():\n",
    "    data = [(25000.0,), (55000.0,), (85000.0,), (125000.0,), (200000.0,)]\n",
    "    d = spark.createDataFrame(data, [\"salary_mid_annual\"])\n",
    "    rows = add_salary_band(d).collect()\n",
    "    expected = [\"Entry Level\", \"Mid Level\", \"Senior Level\", \"Director Level\", \"Executive Level\"]\n",
    "    for row, exp in zip(rows, expected):\n",
    "        assert row[\"salary_band\"] == exp, f\"FAIL: {row['salary_band']} != {exp}\"\n",
    "    print(\"  ✅ test_salary_band PASSED\")\n",
    "\n",
    "def test_get_salary_frequency():\n",
    "    data = [(\"1\", \"Annual\"), (\"2\", \"Hourly\"), (\"3\", \"Annual\")]\n",
    "    d = spark.createDataFrame(data, [\"id\", \"Salary Frequency\"])\n",
    "    result = set(get_salary_frequency(d))\n",
    "    assert result == {\"Annual\", \"Hourly\"}, \"FAIL: salary frequencies\"\n",
    "    print(\"  ✅ test_get_salary_frequency PASSED\")\n",
    "\n",
    "# Run all tests\n",
    "test_clean_string_column()\n",
    "test_annual_salary_midpoint()\n",
    "test_degree_level_extraction()\n",
    "test_salary_band()\n",
    "test_get_salary_frequency()\n",
    "\n",
    "print(\"\\n🎉 ALL TESTS PASSED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31633504-4d29-4429-a8fb-688b49e8886f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
